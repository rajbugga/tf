{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "# List all databases\n",
    "databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "\n",
    "# List to hold specific table metadata\n",
    "table_metadata = []\n",
    "\n",
    "for db in databases:\n",
    "    db_name = db.databaseName\n",
    "    tables = spark.sql(f\"SHOW TABLES IN `{db_name}`\").collect()\n",
    "\n",
    "    for table in tables:\n",
    "        table_name = table.tableName\n",
    "\n",
    "        try:\n",
    "            # Use DESCRIBE FORMATTED to get the table type (Managed or External)\n",
    "            describe_formatted_result = spark.sql(f\"DESCRIBE FORMATTED `{db_name}`.`{table_name}`\").collect()\n",
    "            table_type = next((row.data_type.split(':', 1)[1].strip() for row in describe_formatted_result if 'Table Type' in row.col_name), 'Unknown')\n",
    "\n",
    "            table_metadata.append({'schema_name': db_name, 'table_name': table_name, 'table_type': table_type})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing table {db_name}.{table_name}: {e}\")\n",
    "\n",
    "# Convert list of metadata to DataFrame\n",
    "metadata_df = spark.createDataFrame([Row(**metadata) for metadata in table_metadata])\n",
    "\n",
    "# Save the DataFrame as a table named 'inventory'\n",
    "metadata_df.write.mode(\"overwrite\").saveAsTable(\"inventory\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
